{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MeCabの紹介\n",
        "本紹介は，ツールの歴史的背景や詳細な技術的・構成的情報は簡素化し，ツール利用者向けのチュートリアルを指向しています．ご了承ください．"
      ],
      "metadata": {
        "id": "qy3e1QluJv_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MeCabとは\n",
        "MeCabは，日本語文を形態素と呼ばれる単位の語に分割するツール（形態素解析エンジン）です．\n",
        "形態素とは，最小の意味を持つ単位であり，「が」や「に」といった助詞も導けます．\n",
        "MeCabでは，日本語文を単に形態素に分割するだけの「分かち書き」もできますが，品詞なども判別することもできます．\n",
        "\n",
        "このような形態素解析は，旧来より文書間の類似度計算や，文書検索の技術の前処理として利用されてきました．とはいえ，AI技術により自然言語処理技術が発展してきた今でも**AI技術に日本語文章を学習させるデータを作る**ために形態素解析は必須です．\n",
        "\n",
        "公式版のMeCabはC++で実装され，2013年に開発がストップしています．一方では，OS (Operating System)や実装言語等の実行環境の進化による不具合を解消するメンテナンスや，MeCabをPythonやCythonから呼び出せるようにしたラッパー実装などが有志により行なわれています．\n",
        "\n",
        "## fugashiによるMeCab入門\n",
        "\n",
        "ここでは，導入が容易であり，処理の高速性を考慮して，MeCabをCythonから呼び出せる**fugashi**を紹介します．また，Mecabでは，日本語文章を形態素に分割するのですが，そのためには日本語の形態素とは具体的に何かを知るために**辞書**が必要です．ここでは，良く知られている辞書の一つであるUniDicを利用します．他にも，UniDicの軽量版であるUniDic-liteや，旧来から利用されているIPADicやJUMANDicがあります．"
      ],
      "metadata": {
        "id": "_hhF1RNH_NBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UniDic辞書を用いるfugashiのインストール\n",
        "!pip install fugashi[unidic]\n",
        "# 辞書(500MB程度)のダウンロード\n",
        "!python -m unidic download"
      ],
      "metadata": {
        "id": "kbV9SikY5uVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fugashiでは，Taggerクラスを通じてMeCabの機能を実行します．Tagger(arg)のargには，MeCabへの実行引数を与えることになります．ここでの\"-Owakati\"は分かち書きを得るための実行引数であり，解析結果の出力が分かち書き（品詞等は除かれる）が得られます．実行引数の一覧は公式的には https://taku910.github.io/mecab/mecab.html にて参照できますが，Webで検索すれば，よりわかりやすい解説サイトが見つかると思います．Taggerクラスのparseメソッドを呼び出して，解析結果となる文字列を得られます．"
      ],
      "metadata": {
        "id": "Gy2FraPQAbXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fugashi import Tagger\n",
        "\n",
        "tagger = Tagger(\"-Owakati\") #-OwakatiはMecabへの実行引数であり，fugashi APIを通じてMecabに渡して実行する．\n",
        "text = \"信州大学工学部は長野市にあります。\"\n",
        "print(tagger.parse(text)) # 分かち書きした結果のみを出力する．"
      ],
      "metadata": {
        "id": "mj8B40M5FK12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taggerに対して引数を与えない場合は，形態素ごとに品詞等の情報を得ることができます．出力内容として何が出力されているかの説明は https://clrd.ninjal.ac.jp/unidic/faq.html に任せますが，比較的わかりやすいところで品詞や語彙素（見出し語），仮名等の情報が含まれます．"
      ],
      "metadata": {
        "id": "lzbLFoPL1c7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fugashi import Tagger\n",
        "\n",
        "tagger = Tagger() #実行引数未指定の場合\n",
        "text = \"信州大学工学部は長野市にあります。\" #解析対象の文例\n",
        "print(tagger.parse(text)) # 3行目で何も指定しない場合は，品詞等の情報も出力する"
      ],
      "metadata": {
        "id": "Mfo_yYgip38-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記でまとめて出力された品詞や語彙素等の情報ですが，分けて得ることもできます．tagger(text)（※textは解析対象の文章）を呼び出すと，品詞等情報を細かく分割して得られるデータが得られます．たとえば品詞分類はpos（やfeature.pos1, feature.pos2, feature.pos3, feature.pos4）で得られますし，語彙素はfeature.lemmaで得られます．"
      ],
      "metadata": {
        "id": "4xpM_ODZ2Vr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fugashi import Tagger\n",
        "\n",
        "tagger = Tagger()\n",
        "text = \"信州大学工学部は長野市にあります。\" #解析対象の文例\n",
        "for word in tagger(text) :\n",
        "  print(word.surface, #表層形\n",
        "        word.feature.pos1, #品詞大分類\n",
        "        word.feature.pos2, #品詞中分類\n",
        "        word.feature.pos3, #品詞小分類\n",
        "        word.feature.pos4, #品詞細分類\n",
        "        word.feature.cType, #活用型\n",
        "        word.feature.cForm, #活用形\n",
        "        word.feature.lForm, #語彙素読み\n",
        "        word.feature.lemma, #語彙素\n",
        "        word.feature.orth, #書字形出現形\n",
        "        word.feature.pron, #発音形出現形\n",
        "        word.feature.orthBase, #書字形基本形\n",
        "        word.feature.pronBase, #発音形基本形\n",
        "        word.feature.goshu, #語種\n",
        "        word.feature.iType, #語頭変化化型\n",
        "        word.feature.iForm, #語頭変化形\n",
        "        word.feature.fType, #語末変化化型\n",
        "        word.feature.fForm, #語末変化形\n",
        "        word.feature.iConType, #語頭変化結合型\n",
        "        word.feature.fConType, #語末変化結合型\n",
        "        word.feature.type, #\n",
        "        word.feature.kana, #仮名形出現形\n",
        "        word.feature.kanaBase, #仮名形基本形\n",
        "        word.feature.form, #語形出現形\n",
        "        word.feature.formBase, #語形基本形\n",
        "        word.feature.aType, #アクセント型\n",
        "        word.feature.aConType, #アクセント結合型\n",
        "        word.feature.aModType, #アクセント修飾型\n",
        "        word.feature.lid, #語彙表ID\n",
        "        word.feature.lemma_id, #語彙素ID\n",
        "        sep=' ')"
      ],
      "metadata": {
        "id": "XW7nsT2-phvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "情報を絞り出力する場合は，たとえば以下のようになります．"
      ],
      "metadata": {
        "id": "zvGHf9gV31BY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fugashi import Tagger\n",
        "\n",
        "tagger = Tagger()\n",
        "text = \"信州大学工学部は長野市にあります。\" #解析対象の文例\n",
        "for word in tagger(text) :\n",
        "  print(word.surface, #表層形\n",
        "        word.feature.pos1, #品詞大分類\n",
        "        word.feature.lemma, #語彙素\n",
        "        sep=' ')"
      ],
      "metadata": {
        "id": "eruDd4WM3MVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MeCabの応用例\n",
        "ここからは形態素解析の応用例として，解析で得られた形態素をカウントしてみます．表層形をカウントしてしまうと，同義の単語にも係わらず活用形の違いから別の単語としてカウントされてしまうことになるため，一般に基本形でカウントするような工夫をします．このような調整を**前処理**と呼ぶことがあり，他にも単体では意味付けが難しい頻出語（例：「は」や「が」といった助詞）を除くようなストップワード処理があります．このような処理は，たとえば文書類似度を計測する等の処理の前処理として旧来から実施されているものになります．"
      ],
      "metadata": {
        "id": "hvTqeo4T36yX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 長文テキストの解析をデモする文章例\n",
        "!wget 'https://www.aozora.gr.jp/cards/000148/files/773_14560.html' -O 'bunko.html' #こころ（青空文庫）"
      ],
      "metadata": {
        "id": "vQ6I56RUq4fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup #マークアップ言語による記述からデータを抽出するためのライブラリ\n",
        "from collections import Counter\n",
        "from fugashi import Tagger\n",
        "\n",
        "#単なるtxtファイルに書かれた文章を扱う場合はBeautifulSoupは不要\n",
        "#そのtxtファイルをopenして文字列を取り出すのみで十分\n",
        "soup = BeautifulSoup(open(\"bunko.html\", encoding=\"shift_jis\"))\n",
        "text = soup.find(\"div\", \"main_text\").text #main_text classのdivタグに本文がある．\n",
        "\n",
        "tagger = Tagger()\n",
        "base_list = []\n",
        "for word in tagger(text) :\n",
        "  base_list.append(word.feature.orthBase)\n",
        "\n",
        "#形態素の書字形基本形を収集したリストに対しカウントし辞書化する\n",
        "count = Counter(base_list)\n",
        "#カウント数を降順として辞書をソートする\n",
        "sorted_count = sorted(count.items(), key = lambda word:word[1], reverse=True)\n",
        "print(dict(sorted_count))\n"
      ],
      "metadata": {
        "id": "kwb50S-OpiOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記コードの出力結果に見られるように，頻出語の上位として``{'の': 5982, 'た': 5523, '。': 4647, 'は': 4181, 'て': 3856, 'に': 3699, '、': 3610, 'を': 3223, 'だ': 3175, 'と': 2698,``のように助詞や句読点が目立ちます．たとえば，当該文書の特徴を捉えようとしたときに，これらの語が多いことは他文書でも同様の可能性があり，文書の特徴を捉えることは困難です．そこで品詞による語のフィルタリングやストップワード処理を実行します．"
      ],
      "metadata": {
        "id": "HtcapAgt7okV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ストップワードとして公開されているテキストを取得\n",
        "!wget 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt' -O 'stopword.txt'"
      ],
      "metadata": {
        "id": "yTIpG-Uy_TrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup #マークアップ言語による記述からデータを抽出するためのライブラリ\n",
        "from collections import Counter\n",
        "from fugashi import Tagger\n",
        "\n",
        "#単なるtxtファイルに書かれた文章を扱う場合はBeautifulSoupは不要です．\n",
        "#そのtxtファイルをopenして文字列を取り出すのみで十分でしょう．\n",
        "soup = BeautifulSoup(open(\"bunko.html\", encoding=\"shift_jis\"))\n",
        "text = soup.find(\"div\", \"main_text\").text #main_text classのdivタグに本文がある．\n",
        "#ストップワードの読み込み，改行区切りでの単語分割\n",
        "stopwords = open(\"stopword.txt\", 'r').read().split('\\n')\n",
        "#カウントする形態素の品詞指定\n",
        "allowed_pos = [\"名詞\"]\n",
        "\n",
        "tagger = Tagger()\n",
        "base_list = []\n",
        "for word in tagger(text) :\n",
        "  if word.feature.orthBase not in stopwords and word.feature.pos1 in allowed_pos:\n",
        "    base_list.append(word.feature.orthBase)\n",
        "\n",
        "#形態素の書字形基本形を収集したリストに対しカウントし辞書化する\n",
        "count = Counter(base_list)\n",
        "#カウント数を降順として辞書をソートする\n",
        "sorted_count = sorted(count.items(), key = lambda word:word[1], reverse=True)\n",
        "print(dict(sorted_count))\n",
        "\n"
      ],
      "metadata": {
        "id": "zTPba4Fy_IHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "名詞による語のフィルタリング，ならびにストップワード処理により，実行結果は```{'先生': 597, None: 437, '奥': 401, '父': 296, '母': 184, '嬢': 168, '顔': 133, '言葉': 124, '二人': 115, '眼': 111, '心': 106```となり，文書の特徴的な語が見えるようになってきました．ここで```None```は文書の抽出時が不適切に行われた結果によるノイズでしょう．また，動詞も取り上げて，より顕著な特徴を捉えようとすることもあります．\n",
        "\n",
        "このような形態素解析の発展的な応用例として，TF-IDF（Term Frequency / Inverse Document Frequency）を計算して文書を特徴づける語として語の重要度を計算することや，LDA (Latent Dirichlet Allocation) 等のトピックモデリング技術により文書を特徴づける語群を検出することがあります　(cf. http://www.mi.u-tokyo.ac.jp/pdf/3-7_language.pdf )．ただし，このような方法は，AI技術による自然言語処理が台頭してきた現代では古典的な手法として紹介されることがあります．"
      ],
      "metadata": {
        "id": "FewfrFDNBFLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 演習\n",
        "1. 手持ちの文書に対し，形態素解析および形態素のカウントを行ない，その結果を示しなさい．\n",
        "1. 解析結果を表やグラフにより可視化し，どのような可視化が有効かを考察しなさい．"
      ],
      "metadata": {
        "id": "8rWCUZaDHXTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 参考文献\n",
        "1. @taku910, MeCab: Yet Another Part-of-Speech and Morphological Analyzer, https://taku910.github.io/mecab/\n",
        "1. @polm, fugashi, https://github.com/polm/fugashi\n",
        "1. PyPI, fugashi 1.3.0, https://pypi.org/project/fugashi/\n",
        "1. @shogo82148, MeCab @shogo82148 flavored: Yet Another Part-of-Speech and Morphological Analyzer, https://shogo82148.github.io/mecab/\n",
        "1. @SamuraiT, mecab-python3, https://github.com/SamuraiT/mecab-python3\n",
        "1. Stefan Behnel, et al., Cython C-Extensions for Python, https://cython.org/\n",
        "1. PyPi, unidic 1.1.0, https://pypi.org/project/unidic/\n",
        "1. 国立国語研究所, UniDic, https://clrd.ninjal.ac.jp/unidic/download.html\n",
        "1. PyPi, unidic-lite 1.0.8, https://pypi.org/project/unidic-lite/\n",
        "1. PyPi, ipadic 1.0.0, https://pypi.org/project/ipadic/\n",
        "1. PyPi, jumandic 1.0.0, https://pypi.org/project/jumandic/\n",
        "1. 青空文庫, 青空文庫, https://www.aozora.gr.jp/\n",
        "1. 東京大学 数理・情報教育研究センター, 3-7 ⾔語・知識, http://www.mi.u-tokyo.ac.jp/pdf/3-7_language.pdf\n",
        "\n"
      ],
      "metadata": {
        "id": "4swSSV4M-KYd"
      }
    }
  ]
}