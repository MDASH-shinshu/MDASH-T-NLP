{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GinZaの紹介\n",
        "本紹介は，ツールの歴史的背景や詳細な技術的・構成的情報は簡素化し，ツール利用者向けのチュートリアルを指向しています．ご了承ください．"
      ],
      "metadata": {
        "id": "qy3e1QluJv_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GinZaとは\n",
        "GinZaは，Universal Dependenciesに基づく日本語自然言語処理ライブラリです．MeCab等と同様に形態素解析もできますが，その解析にAI技術（例：Transformers事前学習モデル）を応用することにより高精度化を達成しています．自動で**固有表現抽出**や**係り受け解析**もできます．\n",
        "\n",
        "ここでUniversal Dependenciesとは，多国語間を横断して一貫性のある注釈を目指した，統語構造の注釈スキームです．平易に言えば，文章構造を分析（形態素解析や品詞分類，係り受け解析など）したときの結果に対し，様々な国が使う言語に依らず同様な注釈（語への品詞付与等）をできるようにしようとする仕組みです．\n",
        "\n",
        "## GinZa入門\n",
        "\n",
        "ここでは，本資料執筆時点のPyPIに掲載済みの最新版である``ja_ginza_electra``を紹介します．本バージョンはTransforms事前学習モデルを導入したもので，精度向上を図ったものになります．なお，執筆時点ではβ版ではあるものの``ja_ginza_electra``よりも高精度な解析結果が得られる``ja_ginza_bert_large``が公開されています．"
      ],
      "metadata": {
        "id": "_hhF1RNH_NBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ginzaのインストール\n",
        "!pip install ja_ginza_electra"
      ],
      "metadata": {
        "id": "kbV9SikY5uVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GinZaの使用例：詳細な説明は次のセルから．\n",
        "import spacy\n",
        "nlp = spacy.load('ja_ginza_electra')\n",
        "doc = nlp('信州大学工学部は長野市にあります。')\n",
        "for sent in doc.sents:\n",
        "    for token in sent:\n",
        "        print(\n",
        "            token.i,\n",
        "            token.orth_,\n",
        "            token.lemma_,\n",
        "            token.norm_,\n",
        "            token.morph.get(\"Reading\"),\n",
        "            token.pos_,\n",
        "            token.morph.get(\"Inflection\"),\n",
        "            token.tag_,\n",
        "            token.la\n",
        "            token.dep_,\n",
        "            token.head.i,\n",
        "        )"
      ],
      "metadata": {
        "id": "mj8B40M5FK12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GinZaは，Universal Dependenciesによる自然言語処理フレームワークであるspaCyを利用して実現されています．そのため，上記のプログラム例では，GinZaの利用はspaCyのライブラリをインポートするところから始まります．なお，プログラミング上はspaCyのAPIを使うことになりますので，APIはspaCyの公式サイト (cf. https://spacy.io/api ) である程度調べることができます．（ある程度と書いているのは，GinZaの実装に依存して得られるデータが決まっている箇所もあるためです．）そして，``spacy.load``関数で引数にGinZaのパッケージ名を指定することでGinZaを読み込みます．このとき得られるデータは，``Language``オブジェクトになります．\n",
        "\n",
        "ここからは，基本的な使い方を紹介します．``Language``オブジェクト（プログラム上では変数nlp）に対して解析したい文字列を与え（例：``nlp(\"私は車を運転します。\")``て，実行します．この実行結果は``Doc``オブジェクト（ここでは変数doc）として返されます．\n",
        "\n",
        "``Doc``オブジェクトはトークン（文章の分割単位：実態は``Token``オブジェクト）のシーケンスを持ちます．このトークンを文単位にまとめて取り出すということが，``doc.senets``プロパティを参照することでできます．このようにして取り出したデータが``Span``オブジェクトです．\n",
        "\n",
        "``Span``オブジェクト（ここでは変数sent）を繰り返し（``for token in sent:``）の中で参照すると，文単位の``Token``オブジェクトのシーケンスに対して処理ができます.\n",
        "さて，``Token``オブジェクトには様々な属性があります．一部はMeCabと重複するような属性もありますので，MeCabの紹介資料も参考にしてください．"
      ],
      "metadata": {
        "id": "Gy2FraPQAbXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "例題プログラム中で参照している属性等について：\n",
        "\n",
        "``i``は当該トークンの文書中のインデックスを表します．\n",
        "\n",
        "``orth_``は表記上の語です．\n",
        "\n",
        "``lemma_``は語の基本形を表します．\n",
        "\n",
        "``norm_``は基本形で表現を正規化したものです．たとえば，「する」を「為る」といった表現に正規化します．GinZaが形態素解析に用いている``SudachiPy``（正確には``Sudachi``）の説明では，正規化では，送り違い，字種，異体字，誤用，縮約を正規化します．言い換えると，本質的には同じ意味を持つ，異なる複数の表記（誤りを含む）を一つの表記に統一する，というイメージを持つとわかりやすいでしょうか．\n",
        "\n",
        "``morph.get(\"Reading\")``は表記上の読み仮名を得ます．\n",
        "\n",
        "``pos_``では，Universal DependenciesのユニバーサルPOSタグに基づく品詞分類を得ます．\n",
        "\n",
        "``morph.get(\"Inflection\")``では，活用の型を得ます．\n",
        "\n",
        "``tag_``では，日本語における品詞分類を得ます．\n",
        "\n",
        "``dep_``では依存関係（文上における役割：たとえば名詞が主語なのか目的語なのかの区別）を得ます．\n",
        "\n",
        "``head.i``では，単語間の係り受け関係において，当該の語が修飾する先の語のインデックスを示しています．たとえば，プログラム例の結果では「信州大学」は「工学部」を修飾し，「工学部」は「あり（ある）」を修飾しています．"
      ],
      "metadata": {
        "id": "wobGtHhSZ-Z7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記のプログラム例の実行結果では，「信州大学」が単語として捉えられています．（形態素とするならば「信州」と「大学」で分かれてもよいでしょう．）その品詞分類を見ると「名詞-**固有名詞**-一般」とあります．このようにGinZaでは固有名詞を捉えることがある程度できるようになっています．"
      ],
      "metadata": {
        "id": "J63Sb5tHY3jd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ginzaコマンドを実行することで CoNLL-U Syntactic Annotation 形式 (cf. https://universaldependencies.org/format.html#syntactic-annotation )で出力を得ることもできます．"
      ],
      "metadata": {
        "id": "sL9NW7khV0j3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#このセルを実行して出力（警告）が得られたら，その最終行が（わかりにくいが）入力フォーム（テキストボックス）になっている．\n",
        "#その入力フォームに解析したい文章を入力すると解析結果が得られる．停止する場合は，このセル左の停止アイコン（四角）を選択する．\n",
        "#漢字変換の確定が入力の確定に誤認識される可能性があるため，解析対象の文字列はコピペで入力した方が良い．\n",
        "!ginza"
      ],
      "metadata": {
        "id": "5zikhm8oW0rK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MeCabの紹介資料で登場した文章例をGinZaにも適用してみます．"
      ],
      "metadata": {
        "id": "ltY9pTFoe5wL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 長文テキストの解析をデモする文章例\n",
        "!wget 'https://www.aozora.gr.jp/cards/000148/files/773_14560.html' -O 'bunko.html' #こころ（青空文庫）"
      ],
      "metadata": {
        "id": "vQ6I56RUq4fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup #マークアップ言語による記述からデータを抽出するためのライブラリ\n",
        "from collections import Counter\n",
        "import spacy\n",
        "\n",
        "#単なるtxtファイルに書かれた文章を扱う場合はBeautifulSoupは不要\n",
        "#そのtxtファイルをopenして文字列を取り出すのみで十分\n",
        "soup = BeautifulSoup(open(\"bunko.html\", encoding=\"shift_jis\"))\n",
        "text = soup.find(\"div\", \"main_text\").text #main_text classのdivタグに本文がある．\n",
        "\n",
        "base_list = []\n",
        "nlp = spacy.load('ja_ginza_electra')\n",
        "\n",
        "text_splits = text.split() # 今回のバージョンでは，GinZaが一度に処理できるのは49149 bytesまでなので文章を適宜分割\n",
        "text_list = [' '.join(text_splits[i:i+40]) for i in range(0, len(text_splits), 40)] # 時間がかかるので空白区切りの40文章ずつを繋げたリストを用意\n",
        "\n",
        "now = 0\n",
        "for text_unit in text_list: #実行完了まで10分弱かかる可能性がある\n",
        "    now += 1\n",
        "    print(\"現在の進捗：\" + str(now) + \"/\" + str(len(text_splits)//40 + 1))\n",
        "    doc = nlp(text_unit)\n",
        "    for sent in doc.sents:\n",
        "        for token in sent:\n",
        "            base_list.append(token.norm_)\n",
        "\n",
        "#語の正規形を収集したリストに対しカウントし辞書化する\n",
        "count = Counter(base_list)\n",
        "#カウント数を降順として辞書をソートする\n",
        "sorted_count = sorted(count.items(), key = lambda word:word[1], reverse=True)\n",
        "print(dict(sorted_count))\n"
      ],
      "metadata": {
        "id": "kwb50S-OpiOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記プログラムの出力結果に見られるように，頻出語の上位として\n",
        "- ``{'の': 6195, 'た': 5661, '。': 4654, 'は': 4186, 'て': 3862, 'に': 3707, '、': 3643, 'を': 3221, 'だ': 3054, 'と': 2692,``\n",
        "\n",
        "が得られました．MeCabの場合，\n",
        "- ``{'の': 5982, 'た': 5523, '。': 4647, 'は': 4181, 'て': 3856, 'に': 3699, '、': 3610, 'を': 3223, 'だ': 3175, 'と': 2698,``\n",
        "\n",
        "となり，MeCabと異なるカウントになっていることからわかるように，文章の分割結果がMeCabと異なっています．単純な文例では気づきにくいですが，アルゴリズムや辞書，モデルにより統計結果が大きく異なりうることに注意しましょう．MeCabと同様に品詞による語のフィルタリングやストップワード処理を実行します．"
      ],
      "metadata": {
        "id": "HtcapAgt7okV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ストップワードとして公開されているテキストを取得\n",
        "!wget 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt' -O 'stopword.txt'"
      ],
      "metadata": {
        "id": "yTIpG-Uy_TrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup #マークアップ言語による記述からデータを抽出するためのライブラリ\n",
        "from collections import Counter\n",
        "import spacy\n",
        "\n",
        "#単なるtxtファイルに書かれた文章を扱う場合はBeautifulSoupは不要です．\n",
        "#そのtxtファイルをopenして文字列を取り出すのみで十分でしょう．\n",
        "soup = BeautifulSoup(open(\"bunko.html\", encoding=\"shift_jis\"))\n",
        "text = soup.find(\"div\", \"main_text\").text #main_text classのdivタグに本文がある．\n",
        "#ストップワードの読み込み，改行区切りでの単語分割\n",
        "stopwords = open(\"stopword.txt\", 'r').read().split('\\n')\n",
        "#カウントする形態素の品詞指定\n",
        "allowed_pos = [\"名詞\"]\n",
        "\n",
        "tag_noun_list = []\n",
        "pos_noun_list = []\n",
        "tag_verb_list = []\n",
        "pos_verb_list = []\n",
        "\n",
        "nlp = spacy.load('ja_ginza_electra')\n",
        "\n",
        "text_splits = text.split() # 今回のバージョンでは，GinZaが一度に処理できるのは49149 bytesまでなので文章を適宜分割\n",
        "text_list = [' '.join(text_splits[i:i+40]) for i in range(0, len(text_splits), 40)] # 時間がかかるので空白区切りの40文章ずつを繋げたリストを用意\n",
        "\n",
        "now = 0\n",
        "for text_unit in text_list: #実行完了まで10分弱かかる可能性がある\n",
        "    now += 1\n",
        "    print(\"現在の進捗：\" + str(now) + \"/\" + str(len(text_splits)//40 + 1))\n",
        "    doc = nlp(text_unit)\n",
        "    for sent in doc.sents:\n",
        "        for token in sent:\n",
        "            if \"名詞\" in token.tag_ : #日本語の品詞分類に名詞という単語が含まれていたら\n",
        "                tag_noun_list.append(token.norm_)\n",
        "            if \"NOUN\" == token.pos_ : #ユニバーサル品詞タグがNOUN（名詞）であったら\n",
        "                pos_noun_list.append(token.norm_)\n",
        "            if \"動詞\" in token.tag_ : #日本語の品詞分類に動詞という単語が含まれていたら\n",
        "                tag_verb_list.append(token.norm_)\n",
        "            if \"VERB\" == token.pos_ : #ユニバーサル品詞タグがVERB（動詞）であったら\n",
        "                pos_verb_list.append(token.norm_)\n",
        "\n",
        "#語の正規形を収集したリストに対しカウントし辞書化する\n",
        "count_tn = Counter(tag_noun_list)\n",
        "count_pn = Counter(pos_noun_list)\n",
        "count_tv = Counter(tag_verb_list)\n",
        "count_pv = Counter(pos_verb_list)\n",
        "#カウント数を降順として辞書をソートする\n",
        "sorted_count = sorted(count_tn.items(), key = lambda word:word[1], reverse=True)\n",
        "print(\"Tag_Noun:\" + str(dict(sorted_count)))\n",
        "sorted_count = sorted(count_pn.items(), key = lambda word:word[1], reverse=True)\n",
        "print(\"Pos_Noun:\" + str(dict(sorted_count)))\n",
        "sorted_count = sorted(count_tv.items(), key = lambda word:word[1], reverse=True)\n",
        "print(\"Tag_Verb:\" + str(dict(sorted_count)))\n",
        "sorted_count = sorted(count_pv.items(), key = lambda word:word[1], reverse=True)\n",
        "print(\"Pos_Verb:\" + str(dict(sorted_count)))\n"
      ],
      "metadata": {
        "id": "zTPba4Fy_IHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "名詞による語のフィルタリング，ならびにストップワード処理により，実行結果は以下のようになりました．\n",
        "\n",
        "- 日本語の品詞分類における名詞：``{'私': 2676, '其れ': 638, '先生': 597, '事': 576, 'k': 411, '物': 401, '時': 390, '奥さん': 386,``\n",
        "- ユニバーサルPOSタグにおける名詞：``{'先生': 597, '事': 576, '物': 411, 'k': 411, '時': 390, '奥さん': 386, '父': 269, '自分': 261,``\n",
        "- 日本語の品詞分類における動詞：``{'た': 5661, 'だ': 3054, '為る': 2042, 'ます': 1675, 'です': 1625, '居る': 1367, 'ない': 1170,``\n",
        "- ユニバーサルPOSタグにおける動詞：``{'居る': 1363, '言う': 1005, '為る': 834, '有る': 811, '成る': 545, '来る': 342, '見る': 294,``\n",
        "\n",
        "\n",
        "（参考）MeCabで収集した名詞：``{'先生': 597, None: 437, '奥': 401, '父': 296, '母': 184, '嬢': 168, '顔': 133, '言葉': 124, '二人': 115, '眼': 111, '心': 106``\n",
        "\n",
        "日本語の品詞分類における名詞では「私」や「其れ」が頻出語として抽出されていますが，ユニバーサルPOSタグにおける名詞では抽出されていません．これは後者が代名詞（PRON）や固有名詞（PROPN）を名詞（NOUN）とは区別して扱っているからです．（注：細かく条件付けすれば，日本語の品詞分類で代名詞や固有名詞は区別できます．今回のような単純な条件付けではこのようになるということです．）また，MeCabでは「奥」と言う語が形態素として得られていましたが，GinZaでは「奥さん」と言う（物語上でより自然な）単語で抽出されています．GinZaでは「k」も名詞で捉えられています．\n",
        "\n",
        "一方，動詞を見ると，日本語の品詞分類では「だ」や「ます」，「です」，「ない」といった確かに動詞ではあるが，意味としては捉えにくいものが挙げられています．ユニバーサルPOSタグでは，「言う」や「有る」，「成る」，「来る」など単独で意味が捉えやすい単語に絞って得られているのも特徴的です．\n",
        "\n",
        "このように，どのような体系のタグで何の品詞を得るかにより，得られる文書の特徴が変わります．旧来では，文書間の類似度計算などにおいては，文書を特徴づけられる語を如何に適切に抽出できるかが一つの重要な課題であり，汎用性を求めることに限界がありました．現代では，AI技術によりシンプルなプログラムで汎用的な特徴抽出が可能になってきています．"
      ],
      "metadata": {
        "id": "FewfrFDNBFLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 単語間の係り受け関係（Syntactic Dependency Relation）の可視化\n",
        "spaCyでは，文を分割した単語間の依存関係を可視化するモジュールとして``displacy``があります．依存関係の可視化は，大量のテキストを処理する場合には，あまり気にする必要はないように思いますが，解析結果の理解が不十分であれば，理解を促進してくれる効果があるように思います．必要に応じて，利用してみましょう．\n",
        "\n",
        "``spacy.displacy.render``メソッドで可視化できます．利用できる引数は少し説明しますが，完全なものは https://spacy.io/usage/visualizers を参考にしてください．\n",
        "\n",
        "styleには'dep'か'ent'か'span'を与えます．'dep'が係り受け関係を描画するモードで，'ent'が固有表現抽出結果を描画するモードになります．'span'は複数語に跨ってラベルづけをするような可視化を提供しますが，ラベルやその範囲は人が指定するようです．また，google colab上で利用するには``jupyter=True``の引数を加えてください．\n"
      ],
      "metadata": {
        "id": "OTBo92SO0ghq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('ja_ginza_electra')\n",
        "doc = nlp('信州大学工学部は長野市にあります。')\n",
        "spacy.displacy.render(doc, style='dep', jupyter=True)"
      ],
      "metadata": {
        "id": "o2usBkWmzQyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "可視化結果の矢印は，被修飾語から修飾語へと伸びています．"
      ],
      "metadata": {
        "id": "IIzineYRQnzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 固有表現（Named Entity）の抽出と可視化\n",
        "``displacy``では，上記と同様な方法で固有表現を可視化することもできます．必要に応じて，利用してみましょう．"
      ],
      "metadata": {
        "id": "yvnhL3bh4YZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('ja_ginza_electra')\n",
        "doc = nlp('ジョンはジブリ映画と秋葉原が大好きです。今日は午後８時にブランドシネマという映画館に行き，マリーと一緒に映画を見る予定です。')\n",
        "\n",
        "spacy.displacy.render(doc, style='ent', jupyter=True)"
      ],
      "metadata": {
        "id": "LLL7F14Q2knd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "プログラム上で利用できるように固有表現を抽出する方法としては，docのentsを参照する方法があります．entsの各要素(ここでは変数``ent``)に対し，``label_``でラベルを得られます．"
      ],
      "metadata": {
        "id": "uCIrjtLp7n8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 固有表現を抽出するプログラム例\n",
        "import spacy\n",
        "nlp = spacy.load('ja_ginza_electra')\n",
        "doc = nlp('ジョンはジブリ映画と秋葉原が大好きです。今日は午後８時にブランドシネマという映画館に行き，マリーと一緒に映画を見る予定です。')\n",
        "for ent in doc.ents:\n",
        "    print(\n",
        "        ent.text,\n",
        "        ent.label_,\n",
        "    )"
      ],
      "metadata": {
        "id": "uXVWeTvc6A55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## おまけ：LLM (Large Language Model)による固有表現（Named Entity）の抽出\n",
        "執筆時点では，spaCyはLLMと連携して動作させることが可能です．ここでは　おまけ　として，その紹介をします．少し導入の敷居が高いかもしれません．まず``spacy-llm``をインストールします．ランタイムは，``ランタイム``メニューから``ランタイムのタイプの変更``を通じてGPU (T4 GPU)にしておいてください．"
      ],
      "metadata": {
        "id": "yX7TAwuPG7Qo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy-llm"
      ],
      "metadata": {
        "id": "xdwxag_J9rz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次にLLMとの連携設定を書いた``config.cfg``と名付けたファイルを作り，下記の内容とします．ここで，本例ではGoogleのPaLM APIを利用することを前提とします．PaLM APIのAPI Keyが必要になりますので， https://developers.generativeai.google/tutorials/setup?hl=ja を経由してAPI Keyを取得してください．``API キーを取得する``を選択し，移動先で``Create API key in new project``を選択する流れでKeyが取得できます．"
      ],
      "metadata": {
        "id": "dFmP5kY2HpjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Config.cfg\n",
        "```\n",
        "[nlp]\n",
        "lang = \"en\"\n",
        "pipeline = [\"llm\"]\n",
        "\n",
        "[components]\n",
        "\n",
        "[components.llm]\n",
        "factory = \"llm\"\n",
        "\n",
        "[components.llm.task]\n",
        "@llm_tasks = \"spacy.NER.v3\"\n",
        "labels = [\"PERSON\", \"ORGANISATION\", \"LOCATION\"]\n",
        "\n",
        "[components.llm.model]\n",
        "@llm_models = \"spacy.PaLM.v1\"\n",
        "config = {\"temperature\": 0.0}\n",
        "```"
      ],
      "metadata": {
        "id": "E2o8dkvOGyMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次に環境変数にPALM_API_KEYという名でAPI Keyを設定する必要があるため，以下の``<API_KEY>``に具体的なキーを与えて実行します．\n",
        "\n",
        "注：API Keyの意図しない漏洩リスクを少しでも減らすため，Google Colabではなく，ローカル環境で実行環境を整えて実行されることを推奨します．一方で，Google Colab上で実行可能なことは実証済みです．"
      ],
      "metadata": {
        "id": "NQSKDadxI9Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env PALM_API_KEY <API_KEY>"
      ],
      "metadata": {
        "id": "CvEV7B7xDzaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "本例では，固有表現（NER）抽出を試みます．config.cfgにおいて``[components.llm.task]``内の``labels = [\"PERSON\", \"ORGANISATION\", \"LOCATION\"]``をラベルとして，固有表現を抽出します．（どうやら現状，ラベルを4種以上にすると上手く抽出できないようです．）"
      ],
      "metadata": {
        "id": "nnJPe-WGKLti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy_llm.util import assemble\n",
        "\n",
        "nlp = assemble(\"config.cfg\") # config.cfgを読み込んでLLMと連携\n",
        "doc = nlp('John loves Ghibli movies and Akihabara. Today we will go to a movie theater called Brando Cinema at 8 p.m. and watch a movie with Marie.')\n",
        "spacy.displacy.render(doc, style='ent', jupyter=True)"
      ],
      "metadata": {
        "id": "WS0NhteR-A_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "結果としては，GhibliとBrando CinemaがORGANISATIONとされ，AkihabaraがLocationとされ，MarieがPERSONとされました．しかし，Johnが上手くPERSONとして認識されません．より精度よく結果を得るには，調整が必要そうです．"
      ],
      "metadata": {
        "id": "AJCfoptMO4CJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 演習\n",
        "1. 手持ちの文書に対し，GinZaを利用し，日本語の品詞分類とユニバーサルPOSタグでどちらが文書の特徴を捉えられているが，頻出語を計算するなどして確認しなさい．\n",
        "1.  手持ちの文書に対し，固有表現抽出を行ない，固有表現が適切に抽出されているか評価しなさい．"
      ],
      "metadata": {
        "id": "8rWCUZaDHXTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 参考文献\n",
        "1. Megagon Labs, GiNZA: 日本語自然言語処理オープンソースライブラリ, https://www.megagon.ai/jp/projects/ginza-install-a-japanese-nlp-library-in-one-step/\n",
        "1. megagonlabs, GiNZA - Japanese NLP Library, https://megagonlabs.github.io/ginza/\n",
        "1. Universal Dependencies contributors, Universal Dependencies, https://universaldependencies.org/\n",
        "1. Universal Dependencies contributors, UD for Japanese, https://universaldependencies.org/ja/index.html\n",
        "1. Explosion, spaCy, https://spacy.io/\n",
        "1. Explosion, spaCy Library Architecture, https://spacy.io/api\n",
        "1. PyPi, SudachiPy 0.6.7, https://pypi.org/project/SudachiPy/\n",
        "1. WorksApplications, SudahiPy, https://github.com/WorksApplications/sudachi.rs/tree/develop/python\n",
        "1. WorksApplications, Sudahi, https://github.com/WorksApplications/Sudachi\n",
        "1. PyPi, spacy-llm 0.6.4, https://pypi.org/project/spacy-llm/\n",
        "1. 青空文庫, 青空文庫, https://www.aozora.gr.jp/\n"
      ],
      "metadata": {
        "id": "4swSSV4M-KYd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kid3EhnF_L7j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}